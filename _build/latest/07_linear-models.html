

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Linear models &#8212; ECE 4420/6420 - latest</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/myfile.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '07_linear-models';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hyperparameter optimization and optimization bias" href="08_hyperparameter-optimization.html" />
    <link rel="prev" title="ColumnTransformer and text data" href="06_column-transformer-text-feats.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    ECE 4420/6420 Machine Learning in Engineering
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_intro.html">Introduction to machine learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_decision-trees.html">Machine learning concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_ml-fundamentals.html">Machine learning fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_kNNs.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_preprocessing-pipelines.html">Preprocessing and <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="06_column-transformer-text-feats.html"><code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and text data</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_hyperparameter-optimization.html">Hyperparameter optimization and optimization bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_classification-metrics.html">Classification evaluation metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_regression-metrics.html">Regression evaluation metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_ensembles.html">Ensemble learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_multiclass-neuralnetwork.html">Multi-learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_unsupervised-learning-pca.html">Unsupervised learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="15_clustering-kmeans.html">Clustering and k-means</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_dbscan_hierarchical.html">DBSCAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_recommender-systems.html">Recommender systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="22_ethics.html">Machine learning fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_feat-importances-xai.html">ML explainability</a></li>
<li class="toctree-l1"><a class="reference internal" href="30_course-review.html">Course review</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="a01_syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="a02_final_project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="a03_setup.html">Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="b01_attribution.html">Attributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="b02_LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/07_linear-models.ipynb" download="07_linear-models.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-linear-model-is-a-straight-line-in-cartesian-coordinate">A linear model is a straight line in Cartesian coordinate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-multiple-features">Generalizing to multiple features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-train-a-linear-regression">How to train a linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-linear-regression">Prediction of linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-exactly-learning">What are we exactly learning?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-we-making-predictions">How are we making predictions?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-for-more-features">Prediction for more features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-a-special-version-of-linearregression"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, a special version of <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-v-s-linearregression"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> v.s. <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-ridge">Why do we need <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#in-another-view">In another view</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-house-price-prediction">Example of house price prediction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-on-the-ames-housing-dataset"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the Ames housing dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-alpha-of-ridge">Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients-and-intercept-in-ridge">Coefficients and intercept in Ridge</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-ridge">True/False: <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break">Break</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients">Interpretation of coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-of-the-coefficients">Sign of the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-magnitude-of-the-coefficients">The magnitude of the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-scaling">Importance of scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false">True/False</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-to-think-about">Questions to think about</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-intuition">Logistic regression intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-for-the-motivating-example">Training data for the motivating example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-coefficients-associated-with-all-features">Learned coefficients associated with all features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-with-learned-weights">Predicting with learned weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-linear-classifier">Components of a linear classifier</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-of-logisticregression">Training of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-is-hard-to-optimize"><code class="docutils literal notranslate"><span class="pre">sign</span></code> is hard to optimize…</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#log-loss">log loss</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-on-the-city-data">Logistic regression on the city data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-learned-parameters">Accessing learned parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-learned-parameters">Prediction with learned parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#raw-scores">Raw scores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-of-logistic-regression">Decision boundary of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameter-of-logistic-regression">Main hyperparameter of logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-probability-scores">Predicting probability scores</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-proba"><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-calculate-these-probabilities">How does logistic regression calculate these probabilities?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function">The sigmoid function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-confident-cases">Least confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#most-confident-cases">Most confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#overconfident-cases">Overconfident cases</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">True/False</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm-support-vector-machine">Linear SVM (Support Vector Machine)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-hard-margin">SVM with Hard Margin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-soft-margin">SVM with soft margin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-soft-margin-in-an-erm-view">SVM with soft margin in an ERM view</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-svm-erm-empirical-risk-minimization">Logistic Regression, SVM, ERM (Empirical Risk Minimization)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-svm">Prediction of SVM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-trick-in-svm-non-linear-transformations">Kernel trick in SVM - non-linear transformations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#poly-kernel-for-1d">poly kernel for 1D</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-in-sklearn-svc">SVM in sklearn (SVC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-for-regression-svr">SVM for regression (SVR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Break</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-model-interpretation-for-linear-classifiers">Example of model interpretation for linear classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-the-vocabulary">Examining the vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-building-on-the-dataset">Model building on the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-learned-coefficients">Examining learned coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-positive-review">Most positive review</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-negative-review">Most negative review</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#question-for-you-to-ponder-on">Question for you to ponder on</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-linear-models">Summary of linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">Main hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients-in-linear-models">Interpretation of coefficients in linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-of-linear-models">Strengths of linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-linear-models">Limitations of linear models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendation-reading-materials">Recommendation reading materials</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-models">
<h1>Linear models<a class="headerlink" href="#linear-models" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>Update: 2023</p></li>
<li><p>Duration: 150 minutes</p></li>
</ul>
<hr><section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">cmle</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interactive</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span><span class="p">,</span> <span class="n">SVR</span>


<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-outcomes">
<h2>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Permalink to this heading">#</a></h2>
<p>From this lecture, students are expected to be able to:</p>
<ul class="simple">
<li><p>Explain the general intuition behind linear models;</p></li>
<li><p>Explain how <code class="docutils literal notranslate"><span class="pre">predict</span></code> works for linear regression;</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> model;</p></li>
<li><p>Demonstrate how the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> is related to the fundamental tradeoff;</p></li>
<li><p>Explain the difference between linear regression and logistic regression;</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model and <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> to get probability scores</p></li>
<li><p>Explain the advantages of getting probability scores instead of hard predictions during classification;</p></li>
<li><p>Broadly describe linear SVMs</p></li>
<li><p>Explain how can you interpret model predictions using coefficients learned by a linear model;</p></li>
<li><p>Explain the advantages and limitations of linear classifiers</p></li>
<li><p>Carry out multi-class classification using OVR and OVO strategies.</p></li>
</ul>
</section>
<section id="id1">
<h2>Linear models<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p><strong>Linear models</strong> are a fundamental and widely used class of models. They are called <strong>linear</strong> because they make a prediction using a <strong>linear function</strong> of the input features.</p>
<p>We will talk about three linear models:</p>
<ul class="simple">
<li><p>Linear regression</p></li>
<li><p>Logistic regression</p></li>
<li><p>Linear SVM (brief mention)</p></li>
</ul>
<section id="linear-regression">
<h3>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A very popular statistical model and has a long history.</p></li>
<li><p>Imagine a hypothetical regression problem of predicting the weight of a snake given its length.</p></li>
</ul>
<!-- ![snake-weight-length.png](img/snake-weight-length.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/snake-weight-length.png" alt="snake-weight-length.png" width="25%"></center><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">X_1</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">0.2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">])</span>
<span class="n">snakes_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">snakes_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">77</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s2">&quot;length&quot;</span><span class="p">]]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[[</span><span class="s2">&quot;length&quot;</span><span class="p">]]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>73</th>
      <td>1.489130</td>
      <td>10.507995</td>
    </tr>
    <tr>
      <th>53</th>
      <td>1.073233</td>
      <td>7.658047</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1.622709</td>
      <td>9.748797</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.984653</td>
      <td>9.731572</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.484937</td>
      <td>3.016555</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s visualize the hypothetical snake data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight (target)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;weight (target)&#39;)
</pre></div>
</div>
<img alt="_images/ed3f230d36844bf03c27f5da501a89d673bd21476fe1a0a10a2669ed80ee2cb6.png" src="_images/ed3f230d36844bf03c27f5da501a89d673bd21476fe1a0a10a2669ed80ee2cb6.png" />
</div>
</div>
</section>
<section id="a-linear-model-is-a-straight-line-in-cartesian-coordinate">
<h3>A linear model is a straight line in Cartesian coordinate<a class="headerlink" href="#a-linear-model-is-a-straight-line-in-cartesian-coordinate" title="Permalink to this heading">#</a></h3>
<p>Let’s plot a linear function (a straight line) on this dataset.
The line is determined by coefficient and an intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="mf">5.26</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">2.26</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">w</span> <span class="o">*</span> <span class="n">grid</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight (target)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;weight (target)&#39;)
</pre></div>
</div>
<img alt="_images/b32e3d71709545720e4f0cabaeecc51a3442022ffaf79ca309f91f3129b3c1f7.png" src="_images/b32e3d71709545720e4f0cabaeecc51a3442022ffaf79ca309f91f3129b3c1f7.png" />
</div>
</div>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Each line is a linear model.</p></li>
<li><p>w and b have infinite possibilities –&gt; there are infinite possible models.</p></li>
<li><p>Some are good and some are bad.</p></li>
</ul>
</section>
<section id="generalizing-to-multiple-features">
<h3>Generalizing to multiple features<a class="headerlink" href="#generalizing-to-multiple-features" title="Permalink to this heading">#</a></h3>
<p>For more features, the model is a higher dimensional hyperplane and the general formula looks as follows:</p>
<p><span class="math notranslate nohighlight">\(f(\mathbf{x}) =\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_1\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_1\)</span> </font> <span class="math notranslate nohighlight">\(+ \dots +\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_d\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_d\)</span></font> + <font  color="green"> <span class="math notranslate nohighlight">\(b\)</span></font></p>
<p>where,</p>
<ul class="simple">
<li><p><font  color="blue"> (<span class="math notranslate nohighlight">\(x_1, \dots, x_d\)</span>) are input features </font></p></li>
<li><p><font  color="red"> (<span class="math notranslate nohighlight">\(w_1, \dots, w_d\)</span>) are coefficients or weights </font></p></li>
<li><p><font  color="green"> <span class="math notranslate nohighlight">\(b\)</span> is the bias which can be used to offset your hyperplane </font></p></li>
</ul>
<!-- ![multiple-linear-regression-plane.png](img/multiple-linear-regression-plane.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/multiple-linear-regression-plane.png" alt="multiple-linear-regression-plane.png" width="40%"></center></section>
<section id="how-to-train-a-linear-regression">
<h3>How to train a linear regression<a class="headerlink" href="#how-to-train-a-linear-regression" title="Permalink to this heading">#</a></h3>
<p>We make predictions by using
$<span class="math notranslate nohighlight">\( \hat{y}_i = \sum_{i=1}^p w_i x_i +b = \mathbf{w}^T \mathbf{x}_i + b \)</span>$</p>
<p>A good model is the one with the minimal MSE (Mean Squared Error):
$<span class="math notranslate nohighlight">\(\min \sum_{i=1}^n (\hat{y}_i - y_i)^2 \)</span>$</p>
<!-- ![linear-regression-mse.png](img/linear-regression-mse.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/linear-regression-mse.png" alt="linear-regression-mse.png" width="30%"></center><p>Prediction:
$<span class="math notranslate nohighlight">\(\hat{y}_i = \mathbf{w}^T \mathbf{x}_i + b \)</span>$</p>
<p>Minimizing MSE:</p>
<div class="math notranslate nohighlight">
\[\min \sum_{i=1}^n (\hat{y}_i - y_i)^2 \]</div>
<p>If we consider <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> as variables:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{w} \in \mathbb{R}^p, b\in\mathbb{R}} \sum_{i=1}^n ( \mathbf{w}^T\mathbf{x}_i + b - y_i)^2\]</div>
<p>In summary, <code class="docutils literal notranslate"><span class="pre">LinearRegression.fit(X,</span> <span class="pre">y)</span></code> is learning <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> from <code class="docutils literal notranslate"><span class="pre">X,y</span></code>.
In the learned model, <code class="docutils literal notranslate"><span class="pre">coef_</span></code> is <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> is <span class="math notranslate nohighlight">\(b\)</span>.</p>
</section>
<section id="prediction-of-linear-regression">
<h3>Prediction of linear regression<a class="headerlink" href="#prediction-of-linear-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given a snake length, we can use the model above to predict the target (i.e., the weight of the snake).</p></li>
<li><p>The prediction will be the corresponding weight on the orange line.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">r</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">snake_length</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">r</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">snake_length</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<section id="what-are-we-exactly-learning">
<h4>What are we exactly learning?<a class="headerlink" href="#what-are-we-exactly-learning" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The model above is a <strong>line</strong>, which can be represented with a <strong>slope</strong> (i.e., coefficient or weight) and an <strong>intercept</strong>.</p></li>
<li><p>For the above model, we can access the slope (i.e., coefficient or weight) and the intercept using <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>, respectively.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">coef_</span>  <span class="c1"># r is our linear regression object</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5.26370005])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">intercept_</span>  <span class="c1"># r is our linear regression object</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.259057547817184
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="how-are-we-making-predictions">
<h3>How are we making predictions?<a class="headerlink" href="#how-are-we-making-predictions" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given a feature value <span class="math notranslate nohighlight">\(x_1\)</span> and learned coefficient <span class="math notranslate nohighlight">\(w_1\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span>, we can get the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> with the following formula:
$<span class="math notranslate nohighlight">\(\hat{y} = w_1x_1 + b\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">snake_length</span> <span class="o">*</span> <span class="n">r</span><span class="o">.</span><span class="n">coef_</span> <span class="o">+</span> <span class="n">r</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">snake_length</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([6.20683258])
</pre></div>
</div>
</div>
</div>
<p>Great! Now we exactly know how the model is making the prediction.</p>
</section>
<section id="prediction-for-more-features">
<h3>Prediction for more features<a class="headerlink" href="#prediction-for-more-features" title="Permalink to this heading">#</a></h3>
<p>For more features, the model is a higher dimensional hyperplane and the general prediction formula looks as follows:</p>
<p><span class="math notranslate nohighlight">\(\hat{y} =\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_1\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_1\)</span> </font> <span class="math notranslate nohighlight">\(+ \dots +\)</span> <font color="red"><span class="math notranslate nohighlight">\(w_d\)</span></font> <font color="blue"><span class="math notranslate nohighlight">\(x_d\)</span></font> + <font  color="green"> <span class="math notranslate nohighlight">\(b\)</span></font></p>
<p>where,</p>
<ul class="simple">
<li><p><font  color="blue"> (<span class="math notranslate nohighlight">\(x_1, \dots, x_d\)</span>) are input features </font></p></li>
<li><p><font  color="red"> (<span class="math notranslate nohighlight">\(w_1, \dots, w_d\)</span>) are coefficients or weights </font> (learned from the data)</p></li>
<li><p><font  color="green"> <span class="math notranslate nohighlight">\(b\)</span> is the bias which can be used to offset your hyperplane </font> (learned from the data)</p></li>
</ul>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Suppose these are the coefficients learned by a linear regression model on a hypothetical housing price prediction dataset.</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head text-right"><p>Learned coefficient</p></th>
<th class="head text-right"><p>Symbol</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bedrooms</p></td>
<td class="text-right"><p>0.20</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(w_1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Bathrooms</p></td>
<td class="text-right"><p>0.11</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(w_2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Square Footage</p></td>
<td class="text-right"><p>0.002</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(w_3\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Age</p></td>
<td class="text-right"><p>-0.02</p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(w_4\)</span></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Now given a new example, the target will be predicted as follows:
| Bedrooms (<span class="math notranslate nohighlight">\(x_1\)</span>) | Bathrooms (<span class="math notranslate nohighlight">\(x_2\)</span>) | Square Footage (<span class="math notranslate nohighlight">\(x_3\)</span>) | Age (<span class="math notranslate nohighlight">\(x_4\)</span>) |
|——————–|———————|—————-|—–|
| 3                  | 2                   | 1875           | 66  |</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b\]</div>
<div class="math notranslate nohighlight">
\[\text{predicted price}=  0.20 \times 3 + 0.11 \times 2 + 0.002 \times 1875 + (-0.02) \times 66 + b\]</div>
<p>When we call <code class="docutils literal notranslate"><span class="pre">fit</span></code>, a coefficient or weight is learned for each feature which tells us the role of that feature in prediction. These coefficients are learned from the training data.</p>
<p><em><strong>Note</strong></em></p>
<blockquote>
<div><p>In linear models for regression, the model is a line for a single feature, a plane for two features, and a hyperplane for higher dimensions. We are not yet ready to discuss how does linear regression learn these coefficients and intercept.</p>
</div></blockquote>
</section>
<section id="ridge-a-special-version-of-linearregression">
<h3><code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, a special version of <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code><a class="headerlink" href="#ridge-a-special-version-of-linearregression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> has a model called <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> for linear regression.</p></li>
<li><p>But if we use this “vanilla” version of linear regression, it may result in large coefficients and unexpected results.</p></li>
<li><p>So instead of using <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>, we will <em>always use another linear model called <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></em>, which is a linear regression model with a complexity <strong>hyperparameter</strong> <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
</ul>
<section id="ridge-v-s-linearregression">
<h4><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> v.s. <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code><a class="headerlink" href="#ridge-v-s-linearregression" title="Permalink to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>:
$<span class="math notranslate nohighlight">\(\min_{\mathbf{w} \in \mathbb{R}^p, b\in\mathbb{R}} \sum_{i=1}^n ( \mathbf{w}^T\mathbf{x}_i + b - y_i)^2\)</span>$</p>
<p><code class="docutils literal notranslate"><span class="pre">Ridge</span></code>:
$<span class="math notranslate nohighlight">\( \min_{\mathbf{w} \in \mathbb{R}^p, b\in\mathbb{R}} \sum_{i=1}^n (\mathbf{w}^T\mathbf{x}_i + b - y_i)^2 + \alpha ||\mathbf{w}||^2 \)</span>$</p>
<p><strong><span class="math notranslate nohighlight">\(\alpha\)</span> is a hyper-parameter.</strong></p>
</section>
<section id="why-do-we-need-ridge">
<h4>Why do we need <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>?<a class="headerlink" href="#why-do-we-need-ridge" title="Permalink to this heading">#</a></h4>
<!-- ![lr-ridge.png](img/lr-ridge.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/lr-ridge.png" alt="lr-ridge.png" width="85%"></center></section>
<section id="in-another-view">
<h4>In another view<a class="headerlink" href="#in-another-view" title="Permalink to this heading">#</a></h4>
<!-- ![lr-ridge-complexity.png](img/lr-ridge-complexity.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/lr-ridge-complexity.png" alt="lr-ridge-complexity.png" width="45%"></center>
<ul class="simple">
<li><p>Ways to increase complexity</p>
<ul>
<li><p>Add features</p></li>
</ul>
</li>
<li><p>Ways to decrease complexity</p>
<ul>
<li><p>Remove features</p></li>
</ul>
</li>
</ul>
</section>
<section id="example-of-house-price-prediction">
<h4>Example of house price prediction<a class="headerlink" href="#example-of-house-price-prediction" title="Permalink to this heading">#</a></h4>
<p>We will use a dataset of residential homes in Ames, Iowa. Using the non-categorical data in this dataset, we attempt to <strong>predict the final price</strong> of each home.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The Ames housing dataset</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="n">ames</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;house_prices&quot;</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To simplify our demonstration here, we will keep most of the <em>numerical columns</em> and drop the rest of the (categorical) columns. This simplifies column preprocessing and our upcoming analysis in this exercise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ames_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ames</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">ames</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Id&#39;</span><span class="p">)</span>
<span class="c1"># drop non-numerical cols, see note above</span>
<span class="n">ames_df</span> <span class="o">=</span> <span class="n">ames_df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="s1">&#39;number&#39;</span><span class="p">)</span>
<span class="c1"># drop more cols, see note above</span>
<span class="n">ames_df</span> <span class="o">=</span> <span class="n">ames_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MSSubClass&#39;</span><span class="p">,</span> <span class="s1">&#39;MoSold&#39;</span><span class="p">,</span> <span class="s1">&#39;YrSold&#39;</span><span class="p">])</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">ames_df</span><span class="p">,</span> <span class="n">ames</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="c1"># see the train and test sizes</span>
<span class="p">[</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(1168, 33), (292, 33), (1168,), (292,)]
</pre></div>
</div>
</div>
</div>
<p>An extended description of the Ames housing dataset is accessible through <code class="docutils literal notranslate"><span class="pre">ames.url</span></code> and <code class="docutils literal notranslate"><span class="pre">ames.DESCR</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ames housing dataset link:&quot;</span><span class="p">,</span> <span class="n">ames</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ames housing dataset description:</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="n">ames</span><span class="o">.</span><span class="n">DESCR</span><span class="p">[:</span><span class="mi">480</span><span class="p">])</span>  <span class="c1"># print only the introduction</span>
<span class="c1"># print(&quot;Ames housing dataset description:\n\n&quot;, ames.DESCR)  # print the full description</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ames housing dataset link: https://www.openml.org/d/42165
Ames housing dataset description:

 Ask a home buyer to describe their dream house, and they probably won&#39;t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition&#39;s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.
</pre></div>
</div>
</div>
</div>
</section>
<section id="ridge-on-the-ames-housing-dataset">
<h4><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the Ames housing dataset<a class="headerlink" href="#ridge-on-the-ames-housing-dataset" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>  <span class="c1"># DO NOT USE IT</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>  <span class="c1"># USE THIS INSTEAD</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">())</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      fit_time  score_time  test_score  train_score
mean   0.00595    0.001996    0.746312     0.811416
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.006455</td>
      <td>0.001711</td>
      <td>0.805851</td>
      <td>0.800275</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.005912</td>
      <td>0.002100</td>
      <td>0.585754</td>
      <td>0.835169</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.005808</td>
      <td>0.002068</td>
      <td>0.691621</td>
      <td>0.822150</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.005787</td>
      <td>0.002048</td>
      <td>0.819300</td>
      <td>0.800806</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.005787</td>
      <td>0.002053</td>
      <td>0.829031</td>
      <td>0.798680</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="hyperparameter-alpha-of-ridge">
<h4>Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code><a class="headerlink" href="#hyperparameter-alpha-of-ridge" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Ridge has hyperparameters just like the rest of the models we learned.</p></li>
<li><p>The alpha hyperparameter is what makes <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> different from a vanilla <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p></li>
<li><p>Similar to the other hyperparameters that we saw, <code class="docutils literal notranslate"><span class="pre">alpha</span></code> controls the fundamental tradeoff.</p></li>
</ul>
<p><em><strong>Note</strong></em></p>
<blockquote>
<div><p>If we set <code class="docutils literal notranslate"><span class="pre">alpha=0</span></code> that is the same as using LinearRegression.</p>
</div></blockquote>
<p>Let’s examine the effect of <code class="docutils literal notranslate"><span class="pre">alpha</span></code> on the fundamental tradeoff.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;mean_train_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
    <span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;alpha&quot;</span><span class="p">]:</span>
    <span class="n">pipe_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_ridge</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span>
                            <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alpha</th>
      <th>mean_train_scores</th>
      <th>mean_cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.01</td>
      <td>0.811416</td>
      <td>0.746143</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.10</td>
      <td>0.811416</td>
      <td>0.746159</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.00</td>
      <td>0.811416</td>
      <td>0.746312</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10.00</td>
      <td>0.811381</td>
      <td>0.747747</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100.00</td>
      <td>0.809351</td>
      <td>0.756405</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1000.00</td>
      <td>0.773478</td>
      <td>0.750913</td>
    </tr>
    <tr>
      <th>6</th>
      <td>10000.00</td>
      <td>0.479081</td>
      <td>0.472163</td>
    </tr>
    <tr>
      <th>7</th>
      <td>100000.00</td>
      <td>0.089230</td>
      <td>0.076035</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot: xlabel=&#39;alpha&#39;&gt;
</pre></div>
</div>
<img alt="_images/9326da24a7278c517a9ad946f51546eb8d72487b725de52264006ee9c5f2f123.png" src="_images/9326da24a7278c517a9ad946f51546eb8d72487b725de52264006ee9c5f2f123.png" />
</div>
</div>
<p>Thus, <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">100</span></code> is the optimum value here. In general,</p>
<ul class="simple">
<li><p>larger <code class="docutils literal notranslate"><span class="pre">alpha</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> likely to underfit</p></li>
<li><p>smaller <code class="docutils literal notranslate"><span class="pre">alpha</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> likely to overfit</p></li>
</ul>
</section>
<section id="coefficients-and-intercept-in-ridge">
<h4>Coefficients and intercept in Ridge<a class="headerlink" href="#coefficients-and-intercept-in-ridge" title="Permalink to this heading">#</a></h4>
<p>The model learns</p>
<ul class="simple">
<li><p>Coefficients associated with each feature</p></li>
<li><p>The intercept or bias</p></li>
</ul>
<p>Let’s examine the coefficients learned by the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_ridge</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
<span class="n">pipe_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ames_df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficients&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LotFrontage</th>
      <td>822.424355</td>
    </tr>
    <tr>
      <th>LotArea</th>
      <td>3849.729654</td>
    </tr>
    <tr>
      <th>OverallQual</th>
      <td>22319.449000</td>
    </tr>
    <tr>
      <th>OverallCond</th>
      <td>5244.855487</td>
    </tr>
    <tr>
      <th>YearBuilt</th>
      <td>7902.013825</td>
    </tr>
    <tr>
      <th>YearRemodAdd</th>
      <td>3647.923122</td>
    </tr>
    <tr>
      <th>MasVnrArea</th>
      <td>6268.875360</td>
    </tr>
    <tr>
      <th>BsmtFinSF1</th>
      <td>4496.783370</td>
    </tr>
    <tr>
      <th>BsmtFinSF2</th>
      <td>484.156174</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-87.321438</td>
    </tr>
    <tr>
      <th>TotalBsmtSF</th>
      <td>4767.607023</td>
    </tr>
    <tr>
      <th>1stFlrSF</th>
      <td>7863.241209</td>
    </tr>
    <tr>
      <th>2ndFlrSF</th>
      <td>5434.146748</td>
    </tr>
    <tr>
      <th>LowQualFinSF</th>
      <td>-860.068690</td>
    </tr>
    <tr>
      <th>GrLivArea</th>
      <td>10424.733597</td>
    </tr>
    <tr>
      <th>BsmtFullBath</th>
      <td>4316.229111</td>
    </tr>
    <tr>
      <th>BsmtHalfBath</th>
      <td>-319.439242</td>
    </tr>
    <tr>
      <th>FullBath</th>
      <td>1931.799943</td>
    </tr>
    <tr>
      <th>HalfBath</th>
      <td>815.003258</td>
    </tr>
    <tr>
      <th>BedroomAbvGr</th>
      <td>-5965.501937</td>
    </tr>
    <tr>
      <th>KitchenAbvGr</th>
      <td>-5688.739040</td>
    </tr>
    <tr>
      <th>TotRmsAbvGrd</th>
      <td>9970.978889</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>2221.589378</td>
    </tr>
    <tr>
      <th>GarageYrBlt</th>
      <td>764.651491</td>
    </tr>
    <tr>
      <th>GarageCars</th>
      <td>8036.518524</td>
    </tr>
    <tr>
      <th>GarageArea</th>
      <td>805.649891</td>
    </tr>
    <tr>
      <th>WoodDeckSF</th>
      <td>4154.618490</td>
    </tr>
    <tr>
      <th>OpenPorchSF</th>
      <td>-166.083898</td>
    </tr>
    <tr>
      <th>EnclosedPorch</th>
      <td>1383.215119</td>
    </tr>
    <tr>
      <th>3SsnPorch</th>
      <td>711.463720</td>
    </tr>
    <tr>
      <th>ScreenPorch</th>
      <td>3748.859074</td>
    </tr>
    <tr>
      <th>PoolArea</th>
      <td>-6790.963134</td>
    </tr>
    <tr>
      <th>MiscVal</th>
      <td>-235.754353</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><em><strong>Important</strong></em></p>
<blockquote>
<div><p>Take these coefficients with a grain of salt. They might not always match your intuitions.</p>
</div></blockquote>
<ul class="simple">
<li><p>The model also learns an intercept (bias).</p></li>
<li><p>For each prediction, we are adding this amount irrespective of the feature values.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>179876.00428082192
</pre></div>
</div>
</div>
</div>
<p>Can we use this information to interpret model predictions?</p>
</section>
</section>
<section id="questions-for-you">
<h3>Questions for you<a class="headerlink" href="#questions-for-you" title="Permalink to this heading">#</a></h3>
<section id="true-false-ridge">
<h4>True/False: <code class="docutils literal notranslate"><span class="pre">Ridge</span></code><a class="headerlink" href="#true-false-ridge" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Increasing the hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> is likely to decrease model complexity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> can be used with datasets that have multiple features.</p></li>
<li><p>With Ridge, we learn one coefficient per training example.</p></li>
<li><p>If you train a linear regression model on a 2-dimensional problem (2 features), the model will be a two-dimensional plane.</p></li>
</ol>
</section>
</section>
</section>
<section id="break">
<h2>Break<a class="headerlink" href="#break" title="Permalink to this heading">#</a></h2>
</section>
<section id="interpretation-of-coefficients">
<h2>Interpretation of coefficients<a class="headerlink" href="#interpretation-of-coefficients" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>One of the main advantages of linear models is that they are relatively <strong>easy to interpret</strong>.</p></li>
<li><p>We have <strong>one coefficient per feature</strong> which kind of describes the <strong>role of the feature</strong> in the prediction according to the model.</p></li>
</ul>
<p>There are two pieces of information in the coefficients based on</p>
<ul class="simple">
<li><p>Sign</p></li>
<li><p>Magnitude</p></li>
</ul>
<section id="sign-of-the-coefficients">
<h3>Sign of the coefficients<a class="headerlink" href="#sign-of-the-coefficients" title="Permalink to this heading">#</a></h3>
<p>In the example below, for instance:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OverallQual</span></code> (Rates the overall material and finish of the house) has a <strong>positive coefficient</strong>.</p>
<ul>
<li><p>The prediction will be <strong>proportional</strong> to the feature value; as <code class="docutils literal notranslate"><span class="pre">OverallQual</span></code> gets bigger, the median house value gets bigger</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">LowQualFinSF</span></code>: (Low quality finished square feet, all floors) has a <strong>negative coefficient</strong></p>
<ul>
<li><p>The prediction will be <strong>inversely proportional</strong> to the feature value; as <code class="docutils literal notranslate"><span class="pre">LowQualFinSF</span></code> gets bigger, the median house value gets smaller</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ames_df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficients&quot;</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">&#39;OverallQual&#39;</span><span class="p">,</span> <span class="s1">&#39;LowQualFinSF&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>OverallQual</th>
      <td>22319.44900</td>
    </tr>
    <tr>
      <th>LowQualFinSF</th>
      <td>-860.06869</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="the-magnitude-of-the-coefficients">
<h3>The magnitude of the coefficients<a class="headerlink" href="#the-magnitude-of-the-coefficients" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Bigger magnitude <span class="math notranslate nohighlight">\(\rightarrow\)</span> bigger impact on the prediction</p></li>
<li><p>In the example below, both <code class="docutils literal notranslate"><span class="pre">OverallQual</span></code> and <code class="docutils literal notranslate"><span class="pre">Fireplaces</span></code> have a positive impact on the prediction but <code class="docutils literal notranslate"><span class="pre">OverallQual</span></code> would have a bigger positive impact because its feature value is going to be multiplied by a number with a bigger magnitude.</p></li>
<li><p>Similarly, both <code class="docutils literal notranslate"><span class="pre">LowQualFinSF</span></code> and <code class="docutils literal notranslate"><span class="pre">BsmtUnfSF</span></code> have a negative impact on the prediction but <code class="docutils literal notranslate"><span class="pre">LowQualFinSF</span></code> would have a bigger negative impact because it’s going to be multiplied by a number with a bigger magnitude.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;coefficient&quot;</span><span class="p">:</span> <span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;magnitude&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">pipe_ridge</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">tolist</span><span class="p">()),</span>
<span class="p">}</span>
<span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">ames_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span>
    <span class="s2">&quot;magnitude&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">coef_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s1">&#39;OverallQual&#39;</span><span class="p">,</span> <span class="s1">&#39;Fireplaces&#39;</span><span class="p">,</span> <span class="s1">&#39;LowQualFinSF&#39;</span><span class="p">,</span> <span class="s1">&#39;BsmtUnfSF&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coefficient</th>
      <th>magnitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>OverallQual</th>
      <td>22319.449000</td>
      <td>22319.449000</td>
    </tr>
    <tr>
      <th>Fireplaces</th>
      <td>2221.589378</td>
      <td>2221.589378</td>
    </tr>
    <tr>
      <th>LowQualFinSF</th>
      <td>-860.068690</td>
      <td>860.068690</td>
    </tr>
    <tr>
      <th>BsmtUnfSF</th>
      <td>-87.321438</td>
      <td>87.321438</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="importance-of-scaling">
<h3>Importance of scaling<a class="headerlink" href="#importance-of-scaling" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>When you are interpreting the model coefficients, scaling is crucial.</p></li>
<li><p>If you do not scale the data, features with smaller magnitude are going to get coefficients with bigger magnitude whereas features with bigger scale are going to get coefficients with smaller magnitude.</p></li>
<li><p>That said, when you scale the data, feature values become hard to interpret for humans (both sign and magnitude)!</p></li>
</ul>
<p><em><strong>Important</strong></em></p>
<blockquote>
<div><p>Take these coefficients with a grain of salt. They might not always match your intuitions. Also, they do not tell us about how the world works. They only tell us about how the prediction of your model works.</p>
</div></blockquote>
</section>
<section id="id2">
<h3>Questions for you<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<section id="true-false">
<h4>True/False<a class="headerlink" href="#true-false" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>Suppose you have trained a linear model on unscaled data. The coefficients of the linear model have the following interpretation: if coefficient <span class="math notranslate nohighlight">\(j\)</span> is large, that means a change in feature <span class="math notranslate nohighlight">\(j\)</span> has a large impact on the prediction.</p></li>
<li><p>Suppose the scaled feature value of the <code class="docutils literal notranslate"><span class="pre">NOX</span></code> feature above is negative. The prediction will still be inversely proportional to NOX; as NOX gets <strong>bigger</strong>, the median house value gets <strong>smaller</strong>.</p></li>
</ol>
</section>
</section>
<section id="questions-to-think-about">
<h3>Questions to think about<a class="headerlink" href="#questions-to-think-about" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Discuss the importance of scaling when interpreting linear regression coefficients.</p></li>
<li><p>What might be the meaning of a complex vs. simpler model in the case of linear regression?</p></li>
</ul>
</section>
</section>
<section id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h2>
<section id="logistic-regression-intuition">
<h3>Logistic regression intuition<a class="headerlink" href="#logistic-regression-intuition" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>A linear model for <strong>classification</strong>.</p></li>
<li><p>Similar to linear regression, it learns weights associated with each feature and the bias.</p></li>
<li><p>It applies a <strong>threshold</strong> on the raw output to decide whether the class is positive or negative.</p></li>
<li><p>In this lecture, we will focus on the following aspects of logistic regression.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></p></li>
<li><p>how to use learned coefficients to interpret the model</p></li>
</ul>
</li>
</ul>
</section>
<section id="motivating-example">
<h3>Motivating example<a class="headerlink" href="#motivating-example" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Consider the problem of <strong>predicting <em>sentiment</em></strong> expressed in <strong>movie <em>reviews</em></strong>.</p></li>
</ul>
<section id="training-data-for-the-motivating-example">
<h4>Training data for the motivating example<a class="headerlink" href="#training-data-for-the-motivating-example" title="Permalink to this heading">#</a></h4>
<blockquote>
    <p>Review 1: This movie was <b>excellent</b>! The performances were Oscar-worthy!  👍 </p>
    <p>Review 2: What a <b>boring</b> movie! I almost fell asleep twice while watching it. 👎 </p>
    <p>Review 3: I enjoyed the movie. <b>Excellent</b>! 👍 </p>
</blockquote>
<ul class="simple">
<li><p>Targets: positive 👍 and negative 👎</p></li>
<li><p>Features: words (e.g., <em>excellent</em>, <em>flawless</em>, <em>boring</em>)</p></li>
</ul>
</section>
<section id="learned-coefficients-associated-with-all-features">
<h4>Learned coefficients associated with all features<a class="headerlink" href="#learned-coefficients-associated-with-all-features" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Suppose our vocabulary contains only the following 7 words.</p></li>
<li><p>A linear model learns <strong>weights</strong> or <strong>coefficients</strong> associated with the features (words in this example).</p></li>
<li><p>Let’s ignore bias for a bit.</p></li>
</ul>
<!-- ![words_coeff.png](img/words_coeff.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/words_coeff.png" alt="words_coeff.png" width="30%"></center>
</section>
<section id="predicting-with-learned-weights">
<h4>Predicting with learned weights<a class="headerlink" href="#predicting-with-learned-weights" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Use these learned coefficients to make predictions. For example, consider the following review <span class="math notranslate nohighlight">\(x_i\)</span>.</p></li>
</ul>
<blockquote>
It got a bit <b>boring</b> at times but the direction was <b>excellent</b> and the acting was <b>flawless</b>.
</blockquote>
- Feature vector for $x_i$: [1, 0, 1, 1, 0, 0, 0]<!-- ![words_coeff.png](img/words_coeff.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/words_coeff.png" alt="words_coeff.png" width="20%"></center>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(score(x_i) = \)</span> coefficient(<em>boring</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> + coefficient(<em>excellent</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> + coefficient(<em>flawless</em>) <span class="math notranslate nohighlight">\(\times 1\)</span> = <span class="math notranslate nohighlight">\(-1.40 + 1.93 + 1.43 = 1.96\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(1.96 &gt; 0\)</span> so predict the review as positive 👍.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;boring=1&quot;</span><span class="p">,</span> <span class="s2">&quot;excellent=1&quot;</span><span class="p">,</span> <span class="s2">&quot;flawless=1&quot;</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.40</span><span class="p">,</span> <span class="mf">1.93</span><span class="p">,</span> <span class="mf">1.43</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">cmle</span><span class="o">.</span><span class="n">plot_logistic_regression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weighted sum of the input features = 1.960 y_hat = pos
</pre></div>
</div>
<img alt="_images/8130fdb6333a284c1831030747d1ebbc4cbc2634a1fb8f67740c20172b90962f.svg" src="_images/8130fdb6333a284c1831030747d1ebbc4cbc2634a1fb8f67740c20172b90962f.svg" /></div>
</div>
<ul class="simple">
<li><p>So the prediction is based on the weighted sum of the input features.</p></li>
<li><p>Some features are pulling the prediction towards positive sentiment and some are pulling it towards negative sentiment.</p></li>
<li><p>If the coefficient of <em>boring</em> had a bigger magnitude or <em>excellent</em> and <em>flawless</em> had smaller magnitudes, we would have predicted “neg”.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w_0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;boring=1&quot;</span><span class="p">,</span> <span class="s2">&quot;excellent=1&quot;</span><span class="p">,</span> <span class="s2">&quot;flawless=1&quot;</span><span class="p">]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.40</span><span class="p">,</span> <span class="mf">1.93</span><span class="p">,</span> <span class="mf">1.43</span><span class="p">]</span>
    <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_0</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">display</span><span class="p">(</span><span class="n">cmle</span><span class="o">.</span><span class="n">plot_logistic_regression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interactive</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">w_0</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">1.40</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "a5570573636a45e6881d6d1d4943efd6", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>In our case, for values for the coefficient of <em>boring</em> &lt; -3.36, the prediction would be negative.</p>
<p>A linear model learns these coefficients or weights from the training data!</p>
<p>So a linear <strong>classifier</strong> is a linear function of the input <code class="docutils literal notranslate"><span class="pre">X</span></code>, followed by a threshold.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
z =&amp; w_1x_1 + \dots + w_dx_d + b\\
=&amp; \mathbf{w}^T \mathbf{x} + b
\end{split}
\end{equation}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{y} = \begin{cases}
         1, &amp; \text{if } z \geq r\\
         -1, &amp; \text{if } z &lt; r
\end{cases}\end{split}\]</div>
</section>
<section id="components-of-a-linear-classifier">
<h4>Components of a linear classifier<a class="headerlink" href="#components-of-a-linear-classifier" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p>input features (<span class="math notranslate nohighlight">\(x_1, \dots, x_d\)</span>)</p></li>
<li><p>coefficients (weights) (<span class="math notranslate nohighlight">\(w_1, \dots, w_d\)</span>)</p></li>
<li><p>bias (<span class="math notranslate nohighlight">\(b\)</span> or <span class="math notranslate nohighlight">\(w_0\)</span>) (can be used to offset your hyperplane)</p></li>
<li><p>threshold (<span class="math notranslate nohighlight">\(r\)</span>)</p></li>
</ol>
<p>In our example before, we assumed <span class="math notranslate nohighlight">\(r=0\)</span>.</p>
</section>
</section>
<section id="training-of-logisticregression">
<h3>Training of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code><a class="headerlink" href="#training-of-logisticregression" title="Permalink to this heading">#</a></h3>
<p>Recall the prediction:</p>
<div class="math notranslate nohighlight">
\[z_i =\mathbf{w}^T \mathbf{x}_i + b\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{y}_i = \begin{cases}
         1, &amp; \text{if } z_i \geq 0\\
         -1, &amp; \text{if } z_i &lt; 0
\end{cases}
\end{split}\]</div>
<p>Or we can simplify it with a <a class="reference external" href="https://en.wikipedia.org/wiki/Sign_function">sign</a> function:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i = \text{sign}(\mathbf{w} ^T \textbf{x}_i + b)\]</div>
<!-- ![sign.png](img/sign.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/sign.png" alt="sign.png" width="15%"></center><section id="sign-is-hard-to-optimize">
<h4><code class="docutils literal notranslate"><span class="pre">sign</span></code> is hard to optimize…<a class="headerlink" href="#sign-is-hard-to-optimize" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\min_{w \in R^{p}, b \in \mathbb{R}} \sum_{i=1}^n {1}_{[ - y_i \cdot (\mathbf{w}^T \textbf{x}_i + b) ]} \]</div>
<p><span class="math notranslate nohighlight">\(1_{(\cdot)}\)</span> is a <a class="reference external" href="https://en.wikipedia.org/wiki/Indicator_function">indicator</a> function.</p>
<!-- ![binary_loss.png](img/binary_loss.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/binary_loss.png" alt="binary_loss.png" width="40%"></center></section>
<section id="log-loss">
<h4>log loss<a class="headerlink" href="#log-loss" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\min_{w \in R^{p}, b \in \mathbb{R}} \sum_{i=1}^n 1_{[- z_i ]} \]</div>
<div class="math notranslate nohighlight">
\[\min_{w \in ℝ^{p}, b \in \mathbb{R}} \sum_{i=1}^n \log(e^{-z} + 1)\]</div>
<div class="math notranslate nohighlight">
\[ \text{ where } z = y_i \cdot (\mathbf{w}^T \textbf{x}_i + b)\]</div>
<!-- ![binary_loss.png](img/binary_loss.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/binary_loss.png" alt="binary_loss.png" width="30%"></center></section>
</section>
<section id="logistic-regression-on-the-city-data">
<h3>Logistic regression on the city data<a class="headerlink" href="#logistic-regression-on-the-city-data" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmle</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">cols</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>160</th>
      <td>-76.4813</td>
      <td>44.2307</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>127</th>
      <td>-81.2496</td>
      <td>42.9837</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>169</th>
      <td>-66.0580</td>
      <td>45.2788</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>188</th>
      <td>-73.2533</td>
      <td>45.3057</td>
      <td>Canada</td>
    </tr>
    <tr>
      <th>187</th>
      <td>-67.9245</td>
      <td>47.1652</td>
      <td>Canada</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s first try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> on the city data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000814</td>
      <td>0.000879</td>
      <td>0.588235</td>
      <td>0.601504</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000652</td>
      <td>0.000775</td>
      <td>0.588235</td>
      <td>0.601504</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000615</td>
      <td>0.000734</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000589</td>
      <td>0.000556</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000445</td>
      <td>0.000524</td>
      <td>0.606061</td>
      <td>0.597015</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s try <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.004971</td>
      <td>0.000463</td>
      <td>0.852941</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.004014</td>
      <td>0.000433</td>
      <td>0.823529</td>
      <td>0.827068</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.003535</td>
      <td>0.000340</td>
      <td>0.696970</td>
      <td>0.858209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.002679</td>
      <td>0.000358</td>
      <td>0.787879</td>
      <td>0.843284</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.002763</td>
      <td>0.000318</td>
      <td>0.939394</td>
      <td>0.805970</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Logistic regression seems to be doing better than the dummy classifier. But note that there is a lot of variation in the scores.</p>
</section>
<section id="accessing-learned-parameters">
<h3>Accessing learned parameters<a class="headerlink" href="#accessing-learned-parameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Recall that logistic regression learns the weights <span class="math notranslate nohighlight">\(w\)</span> and bias or intercept <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>How to access these weights?</p>
<ul>
<li><p>Similar to <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, we can access the weights and intercept using the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> attributes of the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> object, respectively.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>  <span class="c1"># these are the learned weights</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>  <span class="c1"># this is the bias term</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;features&quot;</span><span class="p">:</span> <span class="n">cols</span><span class="p">,</span> <span class="s2">&quot;coefficients&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.04108149 -0.33683126]]
Model intercept: [10.8869838]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>coefficients</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>longitude</td>
      <td>-0.041081</td>
    </tr>
    <tr>
      <th>1</th>
      <td>latitude</td>
      <td>-0.336831</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Both negative weights</p></li>
<li><p>The weight of latitude is larger in magnitude.</p></li>
<li><p>This makes sense because Canada as a country lies above the USA and so we expect latitude values to contribute more to a prediction than longitude.</p></li>
</ul>
</section>
<section id="prediction-with-learned-parameters">
<h3>Prediction with learned parameters<a class="headerlink" href="#prediction-with-learned-parameters" title="Permalink to this heading">#</a></h3>
<p>Let’s predict the target of a test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-64.8001,  46.098 ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># The country for this example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Canada&#39;
</pre></div>
</div>
</div>
</div>
<section id="raw-scores">
<h4>Raw scores<a class="headerlink" href="#raw-scores" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Calculate the raw score as: <code class="docutils literal notranslate"><span class="pre">y_hat</span> <span class="pre">=</span> <span class="pre">np.dot(w,</span> <span class="pre">x)</span> <span class="pre">+</span> <span class="pre">b</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">))</span> <span class="o">+</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.97817876])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Apply the threshold to the raw score.</p></li>
<li><p>Since the prediction is &lt; 0, predict “negative”.</p></li>
<li><p>What is a “negative” class in our context?</p></li>
<li><p>With logistic regression, the model randomly assigns one of the classes as a positive class and the other as a negative.</p>
<ul>
<li><p>Usually, it would <strong>alphabetically order the target and pick the first one as negative and the second one as a positive class</strong>.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">classes_</span></code> attribute tells us which class is considered negative and which one is considered positive. - In this case, Canada is the negative class and USA is the positive class.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>So based on the negative score above (-1.978), we would predict Canada.</p></li>
<li><p>Let’s check the prediction given by the model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>Great! The predictions match! We exactly know how the model makes predictions.</p>
</section>
</section>
<section id="decision-boundary-of-logistic-regression">
<h3>Decision boundary of logistic regression<a class="headerlink" href="#decision-boundary-of-logistic-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The decision boundary of logistic regression is a <strong>hyperplane</strong> dividing the feature space in half.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;latitude&#39;)
</pre></div>
</div>
<img alt="_images/157176235ca483b438c15d892fb16ac4924dd43c0a5186033f103567bf3d093d.png" src="_images/157176235ca483b438c15d892fb16ac4924dd43c0a5186033f103567bf3d093d.png" />
</div>
</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(d=2\)</span>, the decision boundary is a line (1-dimensional)</p></li>
<li><p>For <span class="math notranslate nohighlight">\(d=3\)</span>, the decision boundary is a plane (2-dimensional)</p></li>
<li><p>For <span class="math notranslate nohighlight">\(d\gt 3\)</span>, the decision boundary is a <span class="math notranslate nohighlight">\(d-1\)</span>-dimensional hyperplane</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="p">[</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)],</span> <span class="n">axes</span>
<span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
        <span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span>
    <span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f90c19f9f30&gt;
</pre></div>
</div>
<img alt="_images/71571e4cf7864250eb4cc2d4b7bab2f03e71654486d9c652d1af0106a66bb36b.png" src="_images/71571e4cf7864250eb4cc2d4b7bab2f03e71654486d9c652d1af0106a66bb36b.png" />
</div>
</div>
<ul class="simple">
<li><p>Notice a linear decision boundary (a line in our case).</p></li>
<li><p>Compare it with  KNN or SVM RBF decision boundaries.</p></li>
</ul>
</section>
<section id="main-hyperparameter-of-logistic-regression">
<h3>Main hyperparameter of logistic regression<a class="headerlink" href="#main-hyperparameter-of-logistic-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">C</span></code> is the main hyperparameter which controls the fundamental trade-off.</p></li>
<li><p>We won’t really talk about the interpretation of this hyperparameter right now.</p></li>
<li><p>At a high level, the interpretation is similar to <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of Linear Regression</p>
<ul>
<li><p>smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> might lead to underfitting</p></li>
<li><p>bigger <code class="docutils literal notranslate"><span class="pre">C</span></code> <span class="math notranslate nohighlight">\(\rightarrow\)</span> might lead to overfitting</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;mean_train_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
    <span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;C&quot;</span><span class="p">]:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
<span class="n">results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>mean_train_scores</th>
      <th>mean_cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0001</td>
      <td>0.664707</td>
      <td>0.658645</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0010</td>
      <td>0.784424</td>
      <td>0.790731</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0100</td>
      <td>0.827842</td>
      <td>0.826203</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.1000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>6</th>
      <td>100.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
    <tr>
      <th>9</th>
      <td>100000.0000</td>
      <td>0.832320</td>
      <td>0.820143</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="predicting-probability-scores">
<h2>Predicting probability scores<a class="headerlink" href="#predicting-probability-scores" title="Permalink to this heading">#</a></h2>
<section id="predict-proba">
<h3><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code><a class="headerlink" href="#predict-proba" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>So far in the context of classification problems, we focused on getting “hard” predictions.</p></li>
<li><p>Very often it’s useful to know “soft” predictions, i.e., how confident the model is with a given prediction.</p></li>
<li><p>For most of the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> classification models, we can access this <strong>confidence score</strong> or <strong>probability score</strong> using a method called <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>.</p></li>
</ul>
<p>Let’s look at the probability scores of the logistic regression model for our test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-64.8001,  46.098 ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>  <span class="c1"># hard prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>  <span class="c1"># soft prediction</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87848688, 0.12151312]])
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The output of <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> is the probability of each class.</p></li>
<li><p>In binary classification, we get probabilities associated with both classes (even though this information is redundant).</p></li>
<li><p>The first entry is the estimated probability of the first class and the second entry is the estimated probability of the second class from <code class="docutils literal notranslate"><span class="pre">model.classes_</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Because it’s a probability, the sum of the entries for both classes should always sum to 1.</p></li>
<li><p>Since the probabilities for the two classes sum to 1, exactly one of the classes will have a score &gt;=0.5, which is going to be our predicted class.</p></li>
</ul>
<section id="how-does-logistic-regression-calculate-these-probabilities">
<h4>How does logistic regression calculate these probabilities?<a class="headerlink" href="#how-does-logistic-regression-calculate-these-probabilities" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The weighted sum <span class="math notranslate nohighlight">\(\mathbf{w}^T \mathbf{x} + b\)</span> gives us “raw model output”.</p></li>
<li><p>For linear regression, this would have been the prediction.</p></li>
<li><p>For logistic regression, you check the <strong>sign</strong> of this value.</p>
<ul>
<li><p>If positive (or 0), predict <span class="math notranslate nohighlight">\(+1\)</span>; if negative, predict <span class="math notranslate nohighlight">\(-1\)</span>.</p></li>
<li><p>These are “hard predictions”.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>You can also have “soft predictions”, aka <strong>predicted probabilities</strong>.</p>
<ul>
<li><p>To convert the raw model output into probabilities, instead of taking the sign, we apply the <strong>sigmoid</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="the-sigmoid-function">
<h4>The sigmoid function<a class="headerlink" href="#the-sigmoid-function" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The sigmoid function “squashes” the raw model output from any number to the range <span class="math notranslate nohighlight">\([0,1]\)</span> using the following formula, where <span class="math notranslate nohighlight">\(x\)</span> is the raw model output.
$<span class="math notranslate nohighlight">\(\frac{1}{1+e^{-x}}\)</span>$</p></li>
<li><p>Then we can interpret the output as probabilities.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="n">raw_model_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">raw_model_output</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s2">&quot;--k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s2">&quot;--k&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;raw model output, $w^Tx$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;the sigmoid function&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;the sigmoid function&#39;)
</pre></div>
</div>
<img alt="_images/1280b04ed0fc731c90139c5b0fd99266c95dd2bdf869be037457989fb264bb8b.png" src="_images/1280b04ed0fc731c90139c5b0fd99266c95dd2bdf869be037457989fb264bb8b.png" />
</div>
</div>
<ul class="simple">
<li><p>Recall our hard predictions that check the sign of <span class="math notranslate nohighlight">\(\mathbf{w}^T \mathbf{x} + b\)</span>, or, in other words, whether or not it is <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<ul>
<li><p>The threshold <span class="math notranslate nohighlight">\(\mathbf{w}^T \mathbf{x} + b=0\)</span> corresponds to <span class="math notranslate nohighlight">\(p=0.5\)</span>.</p></li>
<li><p>In other words, if our predicted probability is <span class="math notranslate nohighlight">\(\geq 0.5\)</span> then our hard prediction is <span class="math notranslate nohighlight">\(+1\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Let’s get the probability score by calling sigmoid on the raw model output for our test example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">))</span> <span class="o">+</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.12151312])
</pre></div>
</div>
</div>
</div>
<p>This is the probability score of the positive class, which is USA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">example</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.87848688, 0.12151312]])
</pre></div>
</div>
</div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, we get the same probability score for USA!!</p>
<ul class="simple">
<li><p>Let’s visualize probability scores for some examples.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">],</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probabilities&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probabilities</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7046068097086478, 0.2953931902913523]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.563016906204013, 0.436983093795987]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.8389680973255862, 0.16103190267441386]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.796415077540433, 0.20358492245956708]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.9010806652340971, 0.09891933476590285]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7753006388010786, 0.22469936119892148]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.030740704606528002, 0.969259295393472]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.6880304799160921, 0.3119695200839079]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.7891358587234142, 0.21086414127658581]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.006546969753885579, 0.9934530302461144]</td>
    </tr>
    <tr>
      <th>10</th>
      <td>USA</td>
      <td>USA</td>
      <td>[0.2787419584843105, 0.7212580415156895]</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>[0.8388877146644935, 0.16111228533550653]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The actual <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">y_hat</span></code> match in most of the cases but in some cases the model is more confident about the prediction than others.</p>
</section>
<section id="least-confident-cases">
<h4>Least confident cases<a class="headerlink" href="#least-confident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is least confident about the prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">least_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">141</span><span class="p">]]</span>
<span class="n">least_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ -79.7599,   43.6858],
       [-123.078 ,   48.9854]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">least_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">127</span><span class="p">,</span> <span class="mi">141</span><span class="p">]]</span>
<span class="n">least_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">least_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">least_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">least_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.634392</td>
      <td>0.365608</td>
    </tr>
    <tr>
      <th>1</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.635666</td>
      <td>0.364334</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">least_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">least_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">least_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4d31a882abf5deb5a1c33e1e2d44925140cdf65a430b91f93bb786a4f2e1f245.png" src="_images/4d31a882abf5deb5a1c33e1e2d44925140cdf65a430b91f93bb786a4f2e1f245.png" />
</div>
</div>
<p>The points are close to the decision boundary which makes sense.</p>
</section>
<section id="most-confident-cases">
<h4>Most confident cases<a class="headerlink" href="#most-confident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is most confident about the prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">37</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">most_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-110.9748,   32.2229],
       [ -67.9245,   47.1652]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">37</span><span class="p">,</span> <span class="mi">165</span><span class="p">]]</span>
<span class="n">most_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;USA&#39;, &#39;Canada&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">most_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">most_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">most_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>USA</td>
      <td>USA</td>
      <td>0.010028</td>
      <td>0.989972</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Canada</td>
      <td>Canada</td>
      <td>0.901081</td>
      <td>0.098919</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-110.9748,   32.2229],
       [ -67.9245,   47.1652]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">most_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">most_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">most_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a4e015b0f673d961ca45743937b985aa71e4c04ccbac5382689c010e3a22bbc4.png" src="_images/a4e015b0f673d961ca45743937b985aa71e4c04ccbac5382689c010e3a22bbc4.png" />
</div>
</div>
<p>The points are far away from the decision boundary which makes sense.</p>
</section>
<section id="overconfident-cases">
<h4>Overconfident cases<a class="headerlink" href="#overconfident-cases" title="Permalink to this heading">#</a></h4>
<p>Let’s examine some cases where the model is confident about the prediction but the prediction is wrong.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">55</span><span class="p">)</span>  <span class="c1"># latitudes above 55</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([25, 55, 98]),)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="p">[[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">98</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Canada&#39;, &#39;USA&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">over_confident_X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">98</span><span class="p">]]</span>
<span class="n">over_confident_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-130.0437,   55.9773],
       [-134.4197,   58.3019]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">over_confident_y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">98</span><span class="p">]]</span>
<span class="n">over_confident_y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;USA&#39;, &#39;USA&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">over_confident_X</span><span class="p">)</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">over_confident_y</span><span class="p">,</span>
    <span class="s2">&quot;y_hat&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">over_confident_X</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s2">&quot;probability score (Canada)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;probability score (USA)&quot;</span><span class="p">:</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>y_hat</th>
      <th>probability score (Canada)</th>
      <th>probability score (USA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.932487</td>
      <td>0.067513</td>
    </tr>
    <tr>
      <th>1</th>
      <td>USA</td>
      <td>Canada</td>
      <td>0.961902</td>
      <td>0.038098</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
    <span class="n">over_confident_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">over_confident_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">over_confident_y</span><span class="p">,</span>
    <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/639fd21b59319ba46ab7f4141497e6e526f765cb3554c092384299b8705354e1.png" src="_images/639fd21b59319ba46ab7f4141497e6e526f765cb3554c092384299b8705354e1.png" />
</div>
</div>
<ul class="simple">
<li><p>The cities are far away from the decision boundary. So the model is pretty confident about the prediction.</p></li>
<li><p>However, the cities are <strong>likely to be from Alaska</strong> and our <strong>linear model is not able to capture that</strong> this part belongs to the USA and not Canada.</p></li>
</ul>
<p>Below we are using colour to represent prediction probabilities. If you are closer to the border, the model is less confident whereas the model is more confident about the mainland cities, which makes sense.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Train class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;Train class 1&quot;</span><span class="p">],</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">))</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">scores_image</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">plot_2d_scores</span><span class="p">(</span>
    <span class="n">lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cm</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span>
<span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">scores_image</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/43ec316793b3eb8355a5f1e9efc830f641e73902a9f47dee6db536c89a821db3.png" src="_images/43ec316793b3eb8355a5f1e9efc830f641e73902a9f47dee6db536c89a821db3.png" />
</div>
</div>
<p>Sometimes a complex model that is overfitted, tends to make more confident predictions, even if they are wrong, whereas a simpler model tends to make predictions with more uncertainty.</p>
<p>To summarize,</p>
<ul class="simple">
<li><p>With hard predictions, we only know the class.</p></li>
<li><p>With probability scores, we know how confident the model is with certain predictions, which can be useful in understanding the model better.</p></li>
</ul>
</section>
</section>
<section id="id3">
<h3>Questions for you<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<section id="id4">
<h4>True/False<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Increasing logistic regression’s <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter increases model complexity.</p></li>
<li><p>Unlike with <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> regression, coefficients are not interpretable with logistic regression.</p></li>
<li><p>The raw output score can be used to calculate the probability score for a given prediction.</p></li>
<li><p>For a linear classifier trained on <span class="math notranslate nohighlight">\(d\)</span> features, the decision boundary is a <span class="math notranslate nohighlight">\(d-1\)</span>-dimensional hyperplane.</p></li>
<li><p>A linear model is likely to be uncertain about the data points close to the decision boundary.</p></li>
<li><p>Similar to decision trees, conceptually logistic regression should be able to work with categorical features.</p></li>
<li><p>Scaling might be a good idea in the context of logistic regression.</p></li>
</ul>
</section>
</section>
</section>
<section id="linear-svm-support-vector-machine">
<h2>Linear SVM (Support Vector Machine)<a class="headerlink" href="#linear-svm-support-vector-machine" title="Permalink to this heading">#</a></h2>
<p>Given a linear-separated example, there might be more than one hyperplane that splits samples.</p>
<!-- ![svm.png](img/svm.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svm.png" alt="svm.png" width="60%"></center>
<p>we select two parallel hyperplanes that separate the two classes of data so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the “margin”.</p>
<section id="svm-with-hard-margin">
<h3>SVM with Hard Margin<a class="headerlink" href="#svm-with-hard-margin" title="Permalink to this heading">#</a></h3>
<p>In the <strong>training</strong> phase, SCM attempts to maximize the margin.</p>
<!-- ![svm_hard_margin.png](img/svm_hard_margin.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svm_hard_margin.png" alt="svm_hard_margin.png" width="30%"></center>
<p>With a normalized or standardized dataset, these hyperplanes can be described by the equations:
<span class="math notranslate nohighlight">\(w^T \textbf{x}_i + b =1\)</span> or <span class="math notranslate nohighlight">\(w^T \textbf{x}_i + b = -1\)</span>.</p>
<p>So $<span class="math notranslate nohighlight">\(\min_{w \in R^{p}, b \in \mathbb{R}}||w||_2^2, \text{ s.t. } y_i(w^T \textbf{x}_i + b) \geq 1 \text{ for } \forall i\)</span>$</p>
</section>
<section id="svm-with-soft-margin">
<h3>SVM with soft margin<a class="headerlink" href="#svm-with-soft-margin" title="Permalink to this heading">#</a></h3>
<p>To extend SVM to cases in which the data are not linearly separable, “errors” are allowed:</p>
<div class="math notranslate nohighlight">
\[\min_{w \in R^{p}, b \in \mathbb{R}}||w||_2^2 + C \sum_{i} \xi_i \]</div>
<div class="math notranslate nohighlight">
\[ \text{ s.t. } y_i(w^T \textbf{x}_i + b) \geq 1 - \xi_i \text{ for } \forall i\]</div>
<p><span class="math notranslate nohighlight">\(\xi_i\)</span> is a slack variable. <span class="math notranslate nohighlight">\(C\)</span> trade-off hyper-parameter.</p>
<!-- ![svm_soft_margin.png](img/svm_soft_margin.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svm_soft_margin.png" alt="svm_soft_margin.png" width="60%"></center>
</section>
<section id="svm-with-soft-margin-in-an-erm-view">
<h3>SVM with soft margin in an ERM view<a class="headerlink" href="#svm-with-soft-margin-in-an-erm-view" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[ y_i(w^T \textbf{x}_i + b) \geq 1 - \xi_i \]</div>
<div class="math notranslate nohighlight">
\[ \xi_i \geq 1 - y_i(w^T \textbf{x}_i + b)  \]</div>
<p>The hinge loss function is helpful to model <span class="math notranslate nohighlight">\(\xi\)</span>:
$<span class="math notranslate nohighlight">\( \max(0,1-y_i(w^T \textbf{x}_i + b))\)</span>$</p>
<p>The soft margin is defined as</p>
<div class="math notranslate nohighlight">
\[\min_{w \in R^{p}, b \in \mathbb{R}} ||w||_2^2 + C \sum_{i=1}^n\max(0,1-y_i(w^T \textbf{x}_i + b)) \]</div>
<!-- ![svm_soft_margin.png](img/svm_soft_margin.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svm_soft_margin.png" alt="svm_soft_margin.png" width="60%"></center>
</section>
<section id="logistic-regression-svm-erm-empirical-risk-minimization">
<h3>Logistic Regression, SVM, ERM (Empirical Risk Minimization)<a class="headerlink" href="#logistic-regression-svm-erm-empirical-risk-minimization" title="Permalink to this heading">#</a></h3>
<!-- ![binary_loss.png](img/binary_loss.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/binary_loss.png" alt="binary_loss.png" width="50%"></center></section>
<section id="prediction-of-svm">
<h3>Prediction of SVM<a class="headerlink" href="#prediction-of-svm" title="Permalink to this heading">#</a></h3>
<p>The same as logistic regression!!</p>
</section>
<section id="kernel-trick-in-svm-non-linear-transformations">
<h3>Kernel trick in SVM - non-linear transformations<a class="headerlink" href="#kernel-trick-in-svm-non-linear-transformations" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Logistic Regression and linear SVM are good for dealing with linear-separated data. But they are not good at non-linear-separated data.</p></li>
<li><p>We can transform the data.</p></li>
</ul>
<section id="poly-kernel-for-1d">
<h4>poly kernel for 1D<a class="headerlink" href="#poly-kernel-for-1d" title="Permalink to this heading">#</a></h4>
<!-- ![svm_kernel_1d.png](img/svm_kernel_1d.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svm_kernel_1d.png" alt="svm_kernel_1d.png" width="60%"></center>
<p>More details about <a class="reference external" href="https://svivek.com/teaching/lectures/slides/svm/kernels.pdf">kernel tricks</a> are available.</p>
</section>
</section>
<section id="svm-in-sklearn-svc">
<h3>SVM in sklearn (SVC)<a class="headerlink" href="#svm-in-sklearn-svc" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cities_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/canada_usa_cities.csv&quot;</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cities_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;country&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;longitude&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SVM RBF&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Linear SVM&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Linear SVM&#39;)
</pre></div>
</div>
<img alt="_images/1a3121d93073e30bab783461ed5dc906a6932a0d85cd5c6904f0f25d08c9bc7b.png" src="_images/1a3121d93073e30bab783461ed5dc906a6932a0d85cd5c6904f0f25d08c9bc7b.png" />
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code> method of linear SVM and logistic regression works the same way.</p></li>
<li><p>We can get <code class="docutils literal notranslate"><span class="pre">coef_</span></code> associated with the features and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> using a Linear SVM model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="n">linear_svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">linear_svc</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">linear_svc</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.0195598  -0.23640124]]
Model intercept: [8.22811601]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model weights: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model intercept: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model weights: [[-0.04108149 -0.33683126]]
Model intercept: [10.8869838]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Note that the coefficients and intercept are slightly different for logistic regression.</p></li>
<li><p>This is because the <code class="docutils literal notranslate"><span class="pre">fit</span></code> for linear SVM and logistic regression are different.</p></li>
</ul>
</section>
<section id="svm-for-regression-svr">
<h3>SVM for regression (SVR)<a class="headerlink" href="#svm-for-regression-svr" title="Permalink to this heading">#</a></h3>
<p>Support Vector Regression (SVR) uses the same principle as SVM but for regression problems.</p>
<!-- ![svr.png](img/svr.png) -->
<center><img src="https://yongkaw.people.clemson.edu/ece4420/img/svr.png" alt="svr.png" width="50%"></center><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ames_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ames</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">ames</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;Id&#39;</span><span class="p">)</span>
<span class="c1"># drop non-numerical cols, see note above</span>
<span class="n">ames_df</span> <span class="o">=</span> <span class="n">ames_df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="s1">&#39;number&#39;</span><span class="p">)</span>
<span class="c1"># drop more cols, see note above</span>
<span class="n">ames_df</span> <span class="o">=</span> <span class="n">ames_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MSSubClass&#39;</span><span class="p">,</span> <span class="s1">&#39;MoSold&#39;</span><span class="p">,</span> <span class="s1">&#39;YrSold&#39;</span><span class="p">])</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">ames_df</span><span class="p">,</span> <span class="n">ames</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">preprocess</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">())</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.046358</td>
      <td>0.010956</td>
      <td>-0.031197</td>
      <td>-0.050938</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.043020</td>
      <td>0.011154</td>
      <td>-0.034481</td>
      <td>-0.050333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.041659</td>
      <td>0.010974</td>
      <td>-0.054221</td>
      <td>-0.058191</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.040644</td>
      <td>0.010772</td>
      <td>-0.115997</td>
      <td>-0.054730</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.040760</td>
      <td>0.012021</td>
      <td>-0.040185</td>
      <td>-0.049034</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="id5">
<h2>Break<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h2>
</section>
<section id="example-of-model-interpretation-for-linear-classifiers">
<h2>Example of model interpretation for linear classifiers<a class="headerlink" href="#example-of-model-interpretation-for-linear-classifiers" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>One of the primary advantages of linear classifiers is their ability to interpret models.</p></li>
<li><p>For example, with the sign and magnitude of learned coefficients, we could answer questions such as which features are driving the prediction in which direction.</p></li>
</ul>
<ul class="simple">
<li><p>We’ll demonstrate this by training <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> on the famous <a class="reference external" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB movie review</a> dataset. The dataset is a bit large for demonstration purposes. So I am going to put a big portion of it in the test split to speed things up.
You can download the data by running the following code.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmle</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;data/imdb_master.csv&quot;</span><span class="p">)</span>
<span class="n">imdb_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/imdb_master.csv&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;ISO-8859-1&quot;</span><span class="p">)</span>
<span class="n">imdb_df</span> <span class="o">=</span> <span class="n">imdb_df</span><span class="p">[</span><span class="n">imdb_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">startswith</span><span class="p">((</span><span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="s2">&quot;neg&quot;</span><span class="p">))]</span>
<span class="n">imdb_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;Unnamed: 0&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="s2">&quot;file&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">imdb_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the charact...</td>
      <td>neg</td>
    </tr>
    <tr>
      <th>1</th>
      <td>This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and...</td>
      <td>neg</td>
    </tr>
    <tr>
      <th>2</th>
      <td>First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like clichÃ©'e version of gangst...</td>
      <td>neg</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Not even the Beatles could write songs everyone liked, and although Walter Hill is no mop-top he's second to none when it comes to thought provoking action movies. The nineties came and social pla...</td>
      <td>neg</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Brass pictures (movies is not a fitting word for them) really are somewhat brassy. Their alluring visual qualities are reminiscent of expensive high class TV commercials. But unfortunately Brass p...</td>
      <td>neg</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s clean up the data a bit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">replace_tags</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;br /&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;https://\S*&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">doc</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imdb_df</span><span class="p">[</span><span class="s2">&quot;review_pp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">imdb_df</span><span class="p">[</span><span class="s2">&quot;review&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">replace_tags</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Are we breaking the Golden rule here?</p>
<p>Let’s split the data and create a bag of word representation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">imdb_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;review_pp&quot;</span><span class="p">],</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;review_pp&quot;</span><span class="p">],</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5000, 3)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10_000</span><span class="p">)</span>
<span class="c1"># select the 1000 most common words in the data</span>
<span class="n">bow</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">bow</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;5000x10000 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 383702 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<section id="examining-the-vocabulary">
<h3>Examining the vocabulary<a class="headerlink" href="#examining-the-vocabulary" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The vocabulary (mapping from feature indices to actual words) can be obtained using <code class="docutils literal notranslate"><span class="pre">get_feature_names_out()</span></code> on the <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> object.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># first few words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;00&#39;, &#39;000&#39;, &#39;01&#39;, &#39;10&#39;, &#39;100&#39;, &#39;1000&#39;, &#39;101&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;],
      dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span><span class="p">[</span><span class="mi">2000</span><span class="p">:</span><span class="mi">2010</span><span class="p">]</span>  <span class="c1"># some middle words</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;conrad&#39;, &#39;cons&#39;, &#39;conscience&#39;, &#39;conscious&#39;, &#39;consciously&#39;,
       &#39;consciousness&#39;, &#39;consequence&#39;, &#39;consequences&#39;, &#39;conservative&#39;,
       &#39;conservatory&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span><span class="p">[::</span><span class="mi">500</span><span class="p">]</span>  <span class="c1"># words with a step of 500</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;00&#39;, &#39;announcement&#39;, &#39;bird&#39;, &#39;cell&#39;, &#39;conrad&#39;, &#39;depth&#39;, &#39;elite&#39;,
       &#39;finnish&#39;, &#39;grimy&#39;, &#39;illusions&#39;, &#39;kerr&#39;, &#39;maltin&#39;, &#39;narrates&#39;,
       &#39;patients&#39;, &#39;publicity&#39;, &#39;reynolds&#39;, &#39;sfx&#39;, &#39;starting&#39;, &#39;thats&#39;,
       &#39;vance&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-building-on-the-dataset">
<h3>Model building on the dataset<a class="headerlink" href="#model-building-on-the-dataset" title="Permalink to this heading">#</a></h3>
<p>First, let’s try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.002818</td>
      <td>0.001843</td>
      <td>0.505</td>
      <td>0.505</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001734</td>
      <td>0.001278</td>
      <td>0.505</td>
      <td>0.505</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.001356</td>
      <td>0.001139</td>
      <td>0.505</td>
      <td>0.505</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.001404</td>
      <td>0.001112</td>
      <td>0.505</td>
      <td>0.505</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.001375</td>
      <td>0.001200</td>
      <td>0.505</td>
      <td>0.505</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We have a balanced dataset. So the <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code> score is around 0.5.</p>
<p>Now let’s try logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10_000</span><span class="p">),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.625504</td>
      <td>0.127574</td>
      <td>0.847</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.627311</td>
      <td>0.129305</td>
      <td>0.832</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.657305</td>
      <td>0.128489</td>
      <td>0.842</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.636376</td>
      <td>0.124130</td>
      <td>0.853</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.642037</td>
      <td>0.126340</td>
      <td>0.839</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Seems like we are overfitting. Let’s optimize the hyperparameter <code class="docutils literal notranslate"><span class="pre">C</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(-3, 0.001), (-2, 0.01), (-1, 0.1), (0, 1.0), (1, 10.0), (2, 100.0)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;mean_train_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
    <span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;C&quot;</span><span class="p">]:</span>
    <span class="n">pipe_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
        <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10_000</span><span class="p">),</span>
        <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_lr</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_train_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;train_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">scores_dict</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s2">&quot;test_score&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_dict</span><span class="p">)</span>
<span class="n">results_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>mean_train_scores</th>
      <th>mean_cv_scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.001</td>
      <td>0.83470</td>
      <td>0.7964</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.010</td>
      <td>0.92265</td>
      <td>0.8456</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.100</td>
      <td>0.98585</td>
      <td>0.8520</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.000</td>
      <td>1.00000</td>
      <td>0.8426</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10.000</td>
      <td>1.00000</td>
      <td>0.8376</td>
    </tr>
    <tr>
      <th>5</th>
      <td>100.000</td>
      <td>1.00000</td>
      <td>0.8350</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimized_C</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;C&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">])]</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;The maximum validation score is </span><span class="si">%0.3f</span><span class="s2"> at C = </span><span class="si">%0.2f</span><span class="s2"> &quot;</span>
    <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">results_df</span><span class="p">[</span><span class="s2">&quot;mean_cv_scores&quot;</span><span class="p">]),</span> <span class="n">optimized_C</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum validation score is 0.852 at C = 0.10 
</pre></div>
</div>
</div>
</div>
<p>Let’s train a model on the full training set with the optimized hyperparameter values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">10_000</span><span class="p">),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">optimized_C</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pipe_lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,
                 CountVectorizer(max_features=10000, stop_words=&#x27;english&#x27;)),
                (&#x27;logisticregression&#x27;,
                 LogisticRegression(C=0.1, max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" ><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,
                 CountVectorizer(max_features=10000, stop_words=&#x27;english&#x27;)),
                (&#x27;logisticregression&#x27;,
                 LogisticRegression(C=0.1, max_iter=1000))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" ><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">CountVectorizer</label><div class="sk-toggleable__content"><pre>CountVectorizer(max_features=10000, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" ><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(C=0.1, max_iter=1000)</pre></div></div></div></div></div></div></div></div></div>
</div>
</section>
<section id="examining-learned-coefficients">
<h3>Examining learned coefficients<a class="headerlink" href="#examining-learned-coefficients" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The learned coefficients are exposed by the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute of <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a> object.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="n">pipe_lr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="n">coeffs</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_coeff_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">coeffs</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficient&quot;</span><span class="p">])</span>
<span class="n">word_coeff_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>00</th>
      <td>-0.074949</td>
    </tr>
    <tr>
      <th>000</th>
      <td>-0.083893</td>
    </tr>
    <tr>
      <th>01</th>
      <td>-0.034402</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.056493</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.041633</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>zoom</th>
      <td>-0.013299</td>
    </tr>
    <tr>
      <th>zooms</th>
      <td>-0.022139</td>
    </tr>
    <tr>
      <th>zorak</th>
      <td>0.021878</td>
    </tr>
    <tr>
      <th>zorro</th>
      <td>0.130075</td>
    </tr>
    <tr>
      <th>â½</th>
      <td>0.012649</td>
    </tr>
  </tbody>
</table>
<p>10000 rows × 1 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>Let’s sort the coefficients in descending order.</p></li>
<li><p>Interpretation</p>
<ul>
<li><p>if <span class="math notranslate nohighlight">\(w_j &gt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward predicting <span class="math notranslate nohighlight">\(+1\)</span>.</p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j &lt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward predicting <span class="math notranslate nohighlight">\(-1\)</span>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_coeff_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;Coefficient&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>excellent</th>
      <td>0.903484</td>
    </tr>
    <tr>
      <th>great</th>
      <td>0.659922</td>
    </tr>
    <tr>
      <th>amazing</th>
      <td>0.653301</td>
    </tr>
    <tr>
      <th>wonderful</th>
      <td>0.651763</td>
    </tr>
    <tr>
      <th>favorite</th>
      <td>0.607887</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>terrible</th>
      <td>-0.621695</td>
    </tr>
    <tr>
      <th>boring</th>
      <td>-0.701030</td>
    </tr>
    <tr>
      <th>bad</th>
      <td>-0.736608</td>
    </tr>
    <tr>
      <th>waste</th>
      <td>-0.799353</td>
    </tr>
    <tr>
      <th>worst</th>
      <td>-0.986970</td>
    </tr>
  </tbody>
</table>
<p>10000 rows × 1 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>The coefficients make sense!</p></li>
</ul>
<p>Let’s visualize the top 20 features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">visualize_coefficients</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45033aeb01ffd28bbfaa4231582df2aaa83e0f4fb11e19d4e0f5ee4ecd284e62.png" src="_images/45033aeb01ffd28bbfaa4231582df2aaa83e0f4fb11e19d4e0f5ee4ecd284e62.png" />
</div>
</div>
<p>Let’s explore the prediction of the following new review.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fake_review</span> <span class="o">=</span> <span class="s2">&quot;It got a bit boring at times but the direction was excellent and the acting was flawless. Overall I enjoyed the movie and I highly recommend it!&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s get the prediction probability scores of the fake review.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">fake_review</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.16423497, 0.83576503]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;neg&#39;, &#39;pos&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>The model is 83.5% confident that it’s a positive review.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">fake_review</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;pos&#39;
</pre></div>
</div>
</div>
</div>
<p>We can find which of the vocabulary words are present in this review:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feat_vec</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">fake_review</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feat_vec</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;1x10000 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 13 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feat_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([False, False, False, ..., False, False, False])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_in_ex</span> <span class="o">=</span> <span class="n">feat_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">words_in_ex</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([False, False, False, ..., False, False, False])
</pre></div>
</div>
</div>
</div>
<p>How many of the words are in this review?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">words_in_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;acting&#39;, &#39;bit&#39;, &#39;boring&#39;, &#39;direction&#39;, &#39;enjoyed&#39;, &#39;excellent&#39;,
       &#39;flawless&#39;, &#39;got&#39;, &#39;highly&#39;, &#39;movie&#39;, &#39;overall&#39;, &#39;recommend&#39;,
       &#39;times&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ex_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">[</span><span class="n">words_in_ex</span><span class="p">],</span>
    <span class="n">index</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficient&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">ex_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>acting</th>
      <td>-0.126498</td>
    </tr>
    <tr>
      <th>bit</th>
      <td>0.390053</td>
    </tr>
    <tr>
      <th>boring</th>
      <td>-0.701030</td>
    </tr>
    <tr>
      <th>direction</th>
      <td>-0.268316</td>
    </tr>
    <tr>
      <th>enjoyed</th>
      <td>0.578879</td>
    </tr>
    <tr>
      <th>excellent</th>
      <td>0.903484</td>
    </tr>
    <tr>
      <th>flawless</th>
      <td>0.113743</td>
    </tr>
    <tr>
      <th>got</th>
      <td>-0.122759</td>
    </tr>
    <tr>
      <th>highly</th>
      <td>0.582012</td>
    </tr>
    <tr>
      <th>movie</th>
      <td>-0.037942</td>
    </tr>
    <tr>
      <th>overall</th>
      <td>0.136288</td>
    </tr>
    <tr>
      <th>recommend</th>
      <td>0.054205</td>
    </tr>
    <tr>
      <th>times</th>
      <td>0.133895</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s visualize how the words with positive and negative coefficients are driving the hard prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">visualize_coefficients</span><span class="p">(</span>
    <span class="n">coeffs</span><span class="p">[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">6</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9c371f45e144bc52d2056cde79aa2c287b1461706be8af3eff3a77f2d3efc106.png" src="_images/9c371f45e144bc52d2056cde79aa2c287b1461706be8af3eff3a77f2d3efc106.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_coeff_example</span><span class="p">(</span><span class="n">feat_vect</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">):</span>
    <span class="n">words_in_ex</span> <span class="o">=</span> <span class="n">feat_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

    <span class="n">ex_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">coeffs</span><span class="p">[</span><span class="n">words_in_ex</span><span class="p">],</span>
        <span class="n">index</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">],</span>
        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Coefficient&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">ex_df</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="most-positive-review">
<h3>Most positive review<a class="headerlink" href="#most-positive-review" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Remember that you can look at the probabilities (confidence) of the classifier’s prediction using the <code class="docutils literal notranslate"><span class="pre">model.predict_proba</span></code> method.</p></li>
<li><p>Can we find the messages where our classifier is most confident or least confident?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos_probs</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1"># only get probabilities associated with pos class</span>
<span class="n">pos_probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.95205899, 0.83301769, 0.9093526 , ..., 0.89247531, 0.05736279,
       0.79360853])
</pre></div>
</div>
</div>
</div>
<p>Let’s get the index of the example where the classifier is most confident (highest <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> score for positive).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_positive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pos_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">most_positive</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Moving beyond words is this heart breaking story of a divorce which results in a tragic custody battle over a seven year old boy.  One of &quot;Kramer v. Kramer\&#39;s&quot; great strengths is its screenwriter director Robert Benton, who has marvellously adapted Avery Corman\&#39;s novel to the big screen. He keeps things beautifully simple and most realistic, while delivering all the drama straight from the heart. His talent for telling emotional tales like this was to prove itself again with &quot;Places in the Heart&quot;, where he showed, as in &quot;Kramer v. Kramer&quot;, that he has a natural ability for working with children.  The picture\&#39;s other strong point is the splendid acting which deservedly received four of the film\&#39;s nine Academy Award nominations, two of them walking away winners. One of those was Dustin Hoffman (Best Actor), who is superb as frustrated business man Ted Kramer, a man who has forgotten that his wife is a person. As said wife Joanne, Meryl Streep claimed the supporting actress Oscar for a strong, sensitive portrayal of a woman who had lost herself in eight years of marriage. Also nominated was Jane Alexander for her fantastic turn as the Kramer\&#39;s good friend Margaret. Final word in the acting stakes must go to young Justin Henry, whose incredibly moving performance will find you choking back tears again and again, and a thoroughly deserved Oscar nomination came his way.  Brilliant also is Nestor Almendros\&#39; cinematography and Jerry Greenberg\&#39;s timely editing, while musically Henry Purcell\&#39;s classical piece is used to effect.  Truly this is a touching story of how a father and son come to depend on each other when their wife and mother leaves. They grow together, come to know each other and form an entirely new and wonderful relationship. Ted finds himself with new responsibilities and a new outlook on life, and slowly comes to realise why Joanne had to go.  Certainly if nothing else, &quot;Kramer v. Kramer&quot; demonstrates that nobody wins when it comes to a custody battle over a young child, especially not the child himself.  Saturday, June 10, 1995 - T.V.  Strong drama from Avery Corman\&#39;s novel about the heartache of a custody battle between estranged parents who both feel they have the child\&#39;s best interests at heart. Aside from a superb screenplay and amazingly controlled direction, both from Robert Benton, it\&#39;s the superlative cast that make this picture such a winner.  Hoffman is brilliant as Ted Kramer, the man torn between his toppling career and the son whom he desperately wants to keep. Excellent too is Streep as the woman lost in eight years of marriage who had to get out before she faded to nothing as a person. In support of these two is a very strong Jane Alexander as mutual friend Margaret, an outstanding Justin Henry as the boy caught in the middle, and a top cast of extras.  This highly emotional, heart rending drama more than deserved it\&#39;s 1979 Academy Awards for best film, best actor (Hoffman) and best supporting actress (Streep).  Wednesday, February 28, 1996 - T.V.&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True target: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">most_positive</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted target: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">most_positive</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction probability: </span><span class="si">%0.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">pos_probs</span><span class="p">[</span><span class="n">most_positive</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True target: pos

Predicted target: pos

Prediction probability: 1.0000
</pre></div>
</div>
</div>
</div>
<p>Let’s examine the features associated with the review.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feat_vec</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
    <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">most_positive</span><span class="p">]])</span>
<span class="n">words_in_ex</span> <span class="o">=</span> <span class="n">feat_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">visualize_coefficients</span><span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b933293ba9fbc58b53a5a3297bc901c1005c1a23be9b227247ec9d408690bd36.png" src="_images/b933293ba9fbc58b53a5a3297bc901c1005c1a23be9b227247ec9d408690bd36.png" />
</div>
</div>
<p>The review has both positive and negative words but the words with <strong>positive</strong> coefficients win in this case!</p>
</section>
<section id="most-negative-review">
<h3>Most negative review<a class="headerlink" href="#most-negative-review" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neg_probs</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="c1"># only get probabilities associated with neg class</span>
<span class="n">neg_probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.04794101, 0.16698231, 0.0906474 , ..., 0.10752469, 0.94263721,
       0.20639147])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">most_negative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">neg_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Review: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">most_negative</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True target: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">most_negative</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted target: </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">most_negative</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction probability: </span><span class="si">%0.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">neg_probs</span><span class="p">[</span><span class="n">most_negative</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Review: 36555    I made the big mistake of actually watching this whole movie a few nights ago. God I&#39;m still trying to recover. This movie does not even deserve a 1.4 average. IMDb needs to have 0 vote ratings po...
Name: review_pp, dtype: object

True target: neg

Predicted target: neg

Prediction probability: 1.0000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feat_vec</span> <span class="o">=</span> <span class="n">pipe_lr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;countvectorizer&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span>
    <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="n">most_negative</span><span class="p">]])</span>
<span class="n">words_in_ex</span> <span class="o">=</span> <span class="n">feat_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">visualize_coefficients</span><span class="p">(</span><span class="n">coeffs</span><span class="p">[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="n">feature_names</span><span class="p">)[</span><span class="n">words_in_ex</span><span class="p">],</span> <span class="n">n_top_features</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/270e27c888790a4ebf8584d3b17b97ca8859071fd254dd8baa9a63302d7f9c40.png" src="_images/270e27c888790a4ebf8584d3b17b97ca8859071fd254dd8baa9a63302d7f9c40.png" />
</div>
</div>
<p>The review has both positive and negative words but the words with negative coefficients win in this case!</p>
</section>
<section id="id6">
<h3>Questions for you<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<section id="question-for-you-to-ponder-on">
<h4>Question for you to ponder on<a class="headerlink" href="#question-for-you-to-ponder-on" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Is it possible to identify the most important features using <span class="math notranslate nohighlight">\(k\)</span>-NNs? What about decision trees?</p></li>
</ul>
</section>
</section>
</section>
<section id="summary-of-linear-models">
<h2>Summary of linear models<a class="headerlink" href="#summary-of-linear-models" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Linear regression is a linear model for regression whereas logistic regression is a linear classification model.</p></li>
<li><p>Both these models learn one coefficient per feature, plus an intercept.</p></li>
</ul>
<section id="main-hyperparameters">
<h3>Main hyperparameters<a class="headerlink" href="#main-hyperparameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The main hyperparameter is the “regularization” hyperparameter controlling the fundamental tradeoff.</p>
<ul>
<li><p>Logistic Regression: <code class="docutils literal notranslate"><span class="pre">C</span></code></p></li>
<li><p>Linear SVM: <code class="docutils literal notranslate"><span class="pre">C</span></code> (SVM-RBF: <code class="docutils literal notranslate"><span class="pre">gamma</span></code>)</p></li>
<li><p>Ridge: <code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="interpretation-of-coefficients-in-linear-models">
<h3>Interpretation of coefficients in linear models<a class="headerlink" href="#interpretation-of-coefficients-in-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(j\)</span>-th coefficient tells us how feature <span class="math notranslate nohighlight">\(j\)</span> affects the prediction</p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j &gt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward predicting <span class="math notranslate nohighlight">\(+1\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j &lt; 0\)</span> then increasing <span class="math notranslate nohighlight">\(x_{ij}\)</span> moves us toward prediction <span class="math notranslate nohighlight">\(-1\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(w_j == 0\)</span> then the feature is not used in making a prediction</p></li>
</ul>
</section>
<section id="strengths-of-linear-models">
<h3>Strengths of linear models<a class="headerlink" href="#strengths-of-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Fast to train and predict</p></li>
<li><p>Scale to large datasets and work well with sparse data</p></li>
<li><p>Relatively easy to understand and interpret the predictions</p></li>
<li><p>Perform well when there is a large number of features</p></li>
</ul>
</section>
<section id="limitations-of-linear-models">
<h3>Limitations of linear models<a class="headerlink" href="#limitations-of-linear-models" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Is your data “linearly separable”? Can you draw a hyperplane between these data points that separates them with 0 errors?</p>
<ul>
<li><p>If the training examples can be separated by a linear decision rule, they are <strong>linearly separable</strong>.</p></li>
</ul>
</li>
</ul>
<p>A few questions you might be thinking about</p>
<ul class="simple">
<li><p>How often the real-life data is linearly separable?</p></li>
<li><p>Is the following XOR function linearly separable?</p></li>
</ul>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>$<span class="math notranslate nohighlight">\(x_1\)</span>$</p></th>
<th class="head"><p>$<span class="math notranslate nohighlight">\(x_2\)</span>$</p></th>
<th class="head"><p>target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Are linear classifiers very limiting because of this?</p></li>
</ul>
</section>
</section>
<section id="recommendation-reading-materials">
<h2>Recommendation reading materials<a class="headerlink" href="#recommendation-reading-materials" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://youtu.be/PaFPbb66DxQ">The Main Ideas of Fitting a Line to Data (The Main Ideas of Least Squares and Linear Regression.)</a></p></li>
<li><p><a class="reference external" href="https://youtu.be/nk2CQITm_eo">Linear Regression, Clearly Explained!!!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=efR1C6CvhmE&amp;list=PLblh5JKOoLUL3IJ4-yor0HzkqDQ3JmJkc">Support Vector Machines Part 1 (of 3): Main Ideas!!!</a></p></li>
</ul>
<hr></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="06_column-transformer-text-feats.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and text data</p>
      </div>
    </a>
    <a class="right-next"
       href="08_hyperparameter-optimization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Hyperparameter optimization and optimization bias</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-linear-model-is-a-straight-line-in-cartesian-coordinate">A linear model is a straight line in Cartesian coordinate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-to-multiple-features">Generalizing to multiple features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-train-a-linear-regression">How to train a linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-linear-regression">Prediction of linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-we-exactly-learning">What are we exactly learning?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-we-making-predictions">How are we making predictions?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-for-more-features">Prediction for more features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-a-special-version-of-linearregression"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, a special version of <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-v-s-linearregression"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> v.s. <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-ridge">Why do we need <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#in-another-view">In another view</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-house-price-prediction">Example of house price prediction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-on-the-ames-housing-dataset"><code class="docutils literal notranslate"><span class="pre">Ridge</span></code> on the Ames housing dataset</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-alpha-of-ridge">Hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> of <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#coefficients-and-intercept-in-ridge">Coefficients and intercept in Ridge</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false-ridge">True/False: <code class="docutils literal notranslate"><span class="pre">Ridge</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break">Break</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients">Interpretation of coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-of-the-coefficients">Sign of the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-magnitude-of-the-coefficients">The magnitude of the coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-of-scaling">Importance of scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#true-false">True/False</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-to-think-about">Questions to think about</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-intuition">Logistic regression intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-data-for-the-motivating-example">Training data for the motivating example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-coefficients-associated-with-all-features">Learned coefficients associated with all features</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-with-learned-weights">Predicting with learned weights</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-linear-classifier">Components of a linear classifier</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-of-logisticregression">Training of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sign-is-hard-to-optimize"><code class="docutils literal notranslate"><span class="pre">sign</span></code> is hard to optimize…</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#log-loss">log loss</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-on-the-city-data">Logistic regression on the city data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-learned-parameters">Accessing learned parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-with-learned-parameters">Prediction with learned parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#raw-scores">Raw scores</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-of-logistic-regression">Decision boundary of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameter-of-logistic-regression">Main hyperparameter of logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-probability-scores">Predicting probability scores</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-proba"><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-calculate-these-probabilities">How does logistic regression calculate these probabilities?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sigmoid-function">The sigmoid function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#least-confident-cases">Least confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#most-confident-cases">Most confident cases</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#overconfident-cases">Overconfident cases</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">True/False</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm-support-vector-machine">Linear SVM (Support Vector Machine)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-hard-margin">SVM with Hard Margin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-soft-margin">SVM with soft margin</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-with-soft-margin-in-an-erm-view">SVM with soft margin in an ERM view</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-svm-erm-empirical-risk-minimization">Logistic Regression, SVM, ERM (Empirical Risk Minimization)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-of-svm">Prediction of SVM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-trick-in-svm-non-linear-transformations">Kernel trick in SVM - non-linear transformations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#poly-kernel-for-1d">poly kernel for 1D</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-in-sklearn-svc">SVM in sklearn (SVC)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svm-for-regression-svr">SVM for regression (SVR)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Break</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-model-interpretation-for-linear-classifiers">Example of model interpretation for linear classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-the-vocabulary">Examining the vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-building-on-the-dataset">Model building on the dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-learned-coefficients">Examining learned coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-positive-review">Most positive review</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-negative-review">Most negative review</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#question-for-you-to-ponder-on">Question for you to ponder on</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-linear-models">Summary of linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-hyperparameters">Main hyperparameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-coefficients-in-linear-models">Interpretation of coefficients in linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#strengths-of-linear-models">Strengths of linear models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-linear-models">Limitations of linear models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendation-reading-materials">Recommendation reading materials</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>